{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c23d502d-d5db-4fa6-9629-c4d1c9373aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "import ast\n",
    "import json\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c033c686-9bf9-433c-b682-be90c37a1325",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tags(tag_string):\n",
    "    \"\"\"\n",
    "    The HTML often shows something like ['Finance'] or ['Combinatorics'].\n",
    "    We'll attempt to parse that as a Python list using ast.literal_eval().\n",
    "    If it fails, we'll just return the raw string.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return ast.literal_eval(tag_string)\n",
    "    except:\n",
    "        return [tag_string]\n",
    "\n",
    "def parse_companies(companies_string):\n",
    "    \"\"\"\n",
    "    Similarly, companies might be something like ['Akuna', 'Citadel'] or [].\n",
    "    We'll parse that too.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return ast.literal_eval(companies_string)\n",
    "    except:\n",
    "        return [companies_string]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46db89ce-4eaa-413c-ad6b-7da6565c616c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_questions = []\n",
    "question_id = 1  # We'll assign incremental IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82614564-383a-441f-9314-79331b530ae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['questions_3.html',\n",
       " 'questions_2.html',\n",
       " 'questions_5.html',\n",
       " 'questions_9.html',\n",
       " 'questions_8.html',\n",
       " 'questions_4.html',\n",
       " 'questions_7.html',\n",
       " 'questions_6.html',\n",
       " 'questions_1.html']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob.glob(\"questions_*.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71f63319-5fd9-4c12-b0c1-72ed24c7f7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing file: questions_3.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nw/54nfvql52953v4t3spj1kfk00000gn/T/ipykernel_3988/4111835250.py:15: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  url_paragraph = div.find(\"p\", text=re.compile(\"URL:\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing file: questions_2.html\n",
      "Parsing file: questions_5.html\n",
      "Parsing file: questions_9.html\n",
      "Parsing file: questions_8.html\n",
      "Parsing file: questions_4.html\n",
      "Parsing file: questions_7.html\n",
      "Parsing file: questions_6.html\n",
      "Parsing file: questions_1.html\n"
     ]
    }
   ],
   "source": [
    "# Loop over all files named questions_1.html, questions_2.html, etc.\n",
    "for filename in glob.glob(\"questions_*.html\"):\n",
    "    print(f\"Parsing file: {filename}\")\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        soup = BeautifulSoup(f, \"html.parser\")\n",
    "\n",
    "        # Each question is inside a <div style='max-width: 65%; ...'>\n",
    "        question_divs = soup.find_all(\"div\", style=re.compile(\"max-width: 65%;\"))\n",
    "        for div in question_divs:\n",
    "            # Extract Title\n",
    "            h2 = div.find(\"h2\")\n",
    "            title = h2.get_text(strip=True) if h2 else \"Untitled\"\n",
    "\n",
    "            # Extract URL (from the <p> that starts with 'URL:')\n",
    "            url_paragraph = div.find(\"p\", text=re.compile(\"URL:\"))\n",
    "            # If we canâ€™t find that exact text, we can just look for the <a> link\n",
    "            a_tag = div.find(\"a\")\n",
    "            url = a_tag.get(\"href\") if a_tag else \"\"\n",
    "\n",
    "            # Extract table info\n",
    "            # Typically, the table has rows: Topic, Tags, Difficulty, Companies, ...\n",
    "            table = div.find(\"table\")\n",
    "            topic = \"\"\n",
    "            tags = []\n",
    "            difficulty = \"\"\n",
    "            companies = []\n",
    "            last_edited_at = \"\"\n",
    "            last_edited_by = \"\"\n",
    "            internal_difficulty = \"\"\n",
    "\n",
    "            if table:\n",
    "                rows = table.find_all(\"tr\")\n",
    "                # We'll rely on the row text, e.g. \"Topic:\", \"Tags:\", ...\n",
    "                for row in rows:\n",
    "                    cols = row.find_all(\"td\")\n",
    "                    if len(cols) == 2:\n",
    "                        key = cols[0].get_text(strip=True).lower()\n",
    "                        val = cols[1].get_text(strip=True)\n",
    "                        if key == \"topic:\":\n",
    "                            topic = val\n",
    "                        elif key == \"tags:\":\n",
    "                            tags = parse_tags(val)\n",
    "                        elif key == \"difficulty:\":\n",
    "                            difficulty = val\n",
    "                        elif key == \"companies:\":\n",
    "                            companies = parse_companies(val)\n",
    "                        elif key == \"last edited at:\":\n",
    "                            last_edited_at = val\n",
    "                        elif key == \"last edited by:\":\n",
    "                            last_edited_by = val\n",
    "                        elif key == \"internal difficulty:\":\n",
    "                            internal_difficulty = val\n",
    "\n",
    "            # Extract the \"Task\" text: <h3>Task</h3> followed by <p> (or multiple <p>).\n",
    "            # In your snippet, there's exactly one <p> for the task (sometimes multiple lines).\n",
    "            # We'll store it as HTML so we can display it later.\n",
    "            taskHTML = \"\"\n",
    "            task_header = div.find(\"h3\", string=re.compile(\"Task\", re.IGNORECASE))\n",
    "            if task_header:\n",
    "                # The next sibling that is a <p> might contain the task\n",
    "                # or sometimes there's more than one <p> until the next <details>.\n",
    "                # For simplicity, let's just grab everything until the next <details>.\n",
    "                task_elems = []\n",
    "                nxt = task_header.find_next_sibling()\n",
    "                while nxt and nxt.name != \"details\":\n",
    "                    # We'll keep appending paragraphs or text\n",
    "                    task_elems.append(str(nxt))\n",
    "                    nxt = nxt.find_next_sibling()\n",
    "                taskHTML = \"\\n\".join(task_elems)\n",
    "\n",
    "            # For the <details> blocks: \"Hint\", \"Solution\", \"Answer\"\n",
    "            # We'll store them as hintHTML, solutionHTML, answerHTML\n",
    "            hintHTML = \"\"\n",
    "            solutionHTML = \"\"\n",
    "            answerHTML = \"\"\n",
    "\n",
    "            details_tags = div.find_all(\"details\")\n",
    "            for dtag in details_tags:\n",
    "                summary = dtag.find(\"summary\")\n",
    "                if summary:\n",
    "                    summary_text = summary.get_text(strip=True).lower()\n",
    "                    # We match by substring\n",
    "                    if \"hint\" in summary_text:\n",
    "                        hintHTML = str(dtag)  # Entire <details> block or just the content\n",
    "                    elif \"solution\" in summary_text:\n",
    "                        solutionHTML = str(dtag)\n",
    "                    elif \"answer\" in summary_text:\n",
    "                        answerHTML = str(dtag)\n",
    "\n",
    "            # Build a question dict\n",
    "            question_data = {\n",
    "                \"id\": question_id,\n",
    "                \"title\": title,\n",
    "                \"url\": url,\n",
    "                \"topic\": topic.lower(),  # normalize\n",
    "                \"tags\": tags,\n",
    "                \"difficulty\": difficulty.lower(),\n",
    "                \"companies\": companies,\n",
    "                \"lastEditedAt\": last_edited_at,\n",
    "                \"lastEditedBy\": last_edited_by,\n",
    "                \"internalDifficulty\": internal_difficulty,\n",
    "                \"taskHTML\": taskHTML.strip(),\n",
    "                \"hintHTML\": hintHTML.strip(),\n",
    "                \"solutionHTML\": solutionHTML.strip(),\n",
    "                \"answerHTML\": answerHTML.strip()\n",
    "            }\n",
    "            all_questions.append(question_data)\n",
    "            question_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "060b19f4-1c38-4242-b560-0493db6e53c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1204"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a3c56e3-6e1c-4f84-abdf-8501be855338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out to JSON\n",
    "with open(\"questionsData.json\", \"w\", encoding=\"utf-8\") as out:\n",
    "    json.dump(all_questions, out, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00b6de2b-b739-4d9e-8849-2764d3690af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Extracted 1204 questions into questionsData.json.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Done! Extracted {len(all_questions)} questions into questionsData.json.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942ae8f4-fa73-4f8d-b9d1-e7f691a80f50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quant",
   "language": "python",
   "name": "quant"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
